(DONE) 1. Install Apache Oozie and Hadoop.
- I have installed the HDP virtual machine.

(DONE) 3. Write down some of the design of the system.
3.1 General design
3.1.1 - Submit to the system a workflow with:
	- each action having input and output defined as parameters.
	- the output of the action can be marked as forceStore, and if given a path as parameter
	it is stored in that specified path (which must be a user path). Otherwise, the system
	attempts to store it on the space the system manages itself if it has space there.
	Of course, it should, because since this is a forceStore dataset, it has priority
	over any dataset that is not forceStore that the system currently has on storage.
	- An action has as input the outputs of the actions it depends on, as well as additional
	inputs specified by the user in parameters. My question here is: How will the program
	identify each of the inputs inside? Do I need to make stronger restrictions and
	assumptions here? Yes, the user programs on the system will need to take as parameter a well
	defined input data file that specifies all this.
		- Defining the input file of the actions:
			- It will just be a list of key-value pairs, where the key is the identification
			name of the input, and the value is a list of paths of folders that have that input. 
			- In order to specify the input of the actions on which this current action depends
			on, the system will use two different ways of presenting that information to the
			current action:
				- "allDependingInput": which will have a list of folders to where all the output of
				the parent actions is.
				- "allDependingInput.actionName": To specifically access the folders of inputs
				produced by one specific action, if it is that that action was given a name
				in the configuration file.
				I am going to use JSON configuration for this system.
				Because I love JSON, and it is much simpler than anything else.
		- Remember that this input file with the configuration of the input to a task
		is generated by the system, since the system is the only one who knows where many of
		this paths are at run time.

3.1.2 What does the system do once a workflow has been submitted:
	3.1.2.1 Keeps track of the submission and update statistics on the submissions.
		- Here I need to define the data model that we will use to represent the submissions and
		the statistics on them, in a way that is useful to be queried by others.  Keep in mind
		that one important thing is to be able to access it as time series data too.
		- One approach that I can follow is simply define the most common usage algorithms that
		will be created, and make an API to access the data, and based on the calls to the API
		then define the data model that will make those calls more efficient to handle, by making
		analysis on run time and frequency of those calls by the algorithms.
	3.1.2.2 Keeps track of the provenance of each dataset.
		- This one is a difficult task. First of all, actions needs to be safe
		to work on this environment. An action is safe if given the same input, it always 
		produces the same output.  Then, the provenance of a dataset is sufficiently defined
		by the parent action and its inputs.  In turn, those inputs could be defined in terms
		of their parent actions and their inputs too. If the assumptions are satisfied, we 
		can say that two datasets are equal if their provenances are equal. This assumption
		needs to be tested on real life to see if the usage of our assumptions is acceptable
		in real life.
			3.1.2.2.1 An important challenge on this is to be able to determine the runtime configuration
			of a task that was run, and of a task that will be run.  Reading Hadoop, the Definitive Guide, 
			in page 159 I found the following information that is important: 
			"More generally, it’s worth knowing that this technique isn’t only useful for failing tasks. 
			You can keep the intermediate files for successful tasks, too, which may be handy if you want
			 to examine a task that isn’t failing. In this case, set the property keep.task.files.pattern
			  to a regular expression that matches the IDs of the tasks you want to keep."
			  
			For me, it is also important to remember that all the configuration parameters that I
			pass to the job, I have control over them, because, at the end, the user only provides
			me with the Map and Reduce classes and the input and output, and I am the one creating a
			TaskRunner class that will set the configuration with the values given by the user.
			That gives me a lot of control on what configuration I will have for that given user.
			
			Also, for third party Java libraries, whose versioning can affect the output of the job
			running, please, check the following entry in Cloudera.
			https://blog.cloudera.com/blog/2011/01/how-to-include-third-party-libraries-in-your-map-reduce-job/
			
			  
	3.1.2.3 An algorithm determines which intermediate outputs from the current workflow submission
	will be stored to the cache.  Now, this here becomes interesting, because I want to separate the
	storing to the cache from the deleting from the cache (if possible)
		3.1.2.3.1 Exploring the option of separating deletion and storing.  Storing needs to be
		done at workflow processing time, because that is the time at which I will have access to the
		data that needs to be stored.  The problem
		3.1.2.3.2 Exploring the option of doing deletion and storing both at workflow processing time.
		I can send to store at workflow processing time by adding an FS action to the generated Oozie
		workflow that attempts to store. If there is no space to store, then I can't
		store, but that's it.  If I want to delete at workflow processing time, I have to be sure that
		no workflow currently submitted to Oozie had a dependency on the dataset I want to delete,
		otherwise, I have to wait until that dependency is removed, and then delete.  The way to do this better
		seems to be: first, send the file to delete to a deletion datastructure that as soon as a file 
		lock is released, it gets notified, and then it places a lock on the file and submits
		a delete action as a workflow to Oozie, and then it releases lock and deletes file registry
		as soon as the deletion action is performed.
			- First thoughts: This is the way to go.
			- An interesting problem to consider here is: how do the real available storage space and the reported
			available storage space may differ so that algorithm decisions of storing space might not be carried
			because there is no sufficient storage. I should make it work so that the available storage space
			input to the algorithm is always less than the real available storage space, and in that way, I solve
			the problem of failure of storing data.  In order to do that, the way to do it is the following:
			Keep two variables: the real system storage available space that is updated each time an FS deletion
			action or FS storage action is executed; the potential system storage available space, which will
			be updated into reduction each time a user submits a workflow, and the decision algorithm decides
			to store a dataset.  The problem with this approach is that we have no clue of how big a dataset
			will be until it is actually produced and stored.  Hence, I will have to deal with uncertainty here
			want it or not, but deal with it we will.  Also, maybe, another solution to the dealing with uncertain-
			ty, in order to have the algorithm as efficient as possible, will be to have it consider that
			the available space is only 95% of the actual available space.  The 95% number should be determined
			by what history says is best.
			
	3.1.2.4 An Oozie workflow generator and submitter that executes the workflow submitted by the system.
	It takes as input the workflow submitted by the user, as well as the output produced by the decision algorithm 
	and then it generates a workflow that executes the DAG of the user, as well as has added FS store actions that
	will also be executed which will essentially store outputs to either the cache or the user storage space.
	
	3.1.2.5 A deletion workflow generator that is working all the time, but that is triggered not by the algorithm
	execution, but by the deletion queue, whenever a lock to a dataset to be deleted is processed.
	
	3.1.2.6 Keeping track of all the FS actions submitted to Oozie in order to update locks to files to 
	delete, as well as available storage space variables.
		  
#This points below might change depending on the need to do them.
(DONE) 4. Create Hadoop tasks that can be controlled to take a certain computation time
and to produce output of a certain size.

(TODO) 5. Create an endpoint to which users can submit workflows.
(TODO) 5. Create endpoints where the users can check the status of workflows.
(DONE) 5. Create a parser for the workflow definition of 3.1.1
(TODO) 6. Create the workflow engine:
(DONE) 6.1 Create a parser for the workflow definition of 3.1.1
(DONE) 6.2 Create an Accountant that updates statistics based on the new workflow submitted to the system.
(TODO) 6.3 Create an Algorithm interface that takes as input the current workflow submission and determines which
intermediate datasets from current workflow will be kept, as well as which datasets from cache should be
sent to the removal queue.
(TODO) 6.3.1 Create a generic historical data access API that will be used by the different algorithm implementations.
(TODO) 6.3.2 Implement a basic classical cache algorithm.
(TODO) 6.3.3 Implement a baseline more complex Knapsack-type algorithm.
(TODO) 6.3.4 Based on the experience designing the algorithms, actually design the data model and write an implementation of
the data access API of 6.3.1 that works fast.
(TODO) 6.3.5 Create a generic computation version control API to be used by the algorithm that has the hard task of comparing
the provenance of filesystem stored datasets with the provenance of datasets generated by the workflow submitted by the user. 
(TODO) 6.3.6 Design the output datastructure of the decision algorithm (it outputs the datasets from the workflow that should
be kept) as well as the datasets from the cache that should be deleted.
(TODO) 6.4 Create the Oozie Workflow generator. Takes as input the workflow submitted by the user, the output of the decision 
algorithm and generates an Oozie workflow.
(TODO) 6.5 Create an Oozie Workflow Submission Manager. Submits workflows to Apache Oozie. Takes care also of updating the
status of a workflow submitted by an user.  It also needs to update the status of the file system capacity given that
the store actions of intermediate datasets are completed. 
(TODO) 6.6 Create a Dataset Deletion Manager.
(TODO) 6.6.1 Create a thread safe queue of filesystem files to be deleted.
(TODO) 6.6.2 Create a manager that deletes the files and handles the locking of files that cannot be deleted.
(TODO) 7. Design the workflow of a workflow submission and use all the systems already designed above:
(TODO) 7.1 Once workflow is submitted: 
		7.1.1 Parse it
		7.1.1 (Return workflow id back to the user?) How slow or fast will the algorithm be? Does this even make any sense?
		7.1.2 Give it to the decision algorithm
		7.1.3 Pass the output of decision algorithm to the Dataset Deletion Manager and to the Oozie Workflow Submission Manager
		7.1.4 Return workflow id back to the user.