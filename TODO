Done 1. Install Apache Oozie and Hadoop.
- I have installed the HDP virtual machine.

2. Create Hadoop tasks that can be controlled to take a certain computation time
and to produce output of a certain size.
- TODO

3. Write down some of the design of the system.
3.1 General design
3.1.1 - Submit to the system a workflow with:
	- each action having input and output defined as parameters.
	- the output of the action can be marked as forceStore, and if given a path as parameter
	it is stored in that specified path (which must be a user path). Otherwise, the system
	attempts to store it on the space the system manages itself if it has space there.
	Of course, it should, because since this is a forceStore dataset, it has priority
	over any dataset that is not forceStore that the system currently has on storage.
	- An action has as input the outputs of the actions it depends on, as well as additional
	inputs specified by the user in parameters. My question here is: How will the program
	identify each of the inputs inside? Do I need to make stronger restrictions and
	assumptions here? Yes, the user programs on the system will need to take as parameter a well
	defined input data file that specifies all this.
		- Defining the input file of the actions:
			- It will just be a collection of key-value pairs, where the key is the identification
			name of the input, and the value is a list of paths of folders that have that input. 
			- In order to specify the input of the actions on which this current action depends
			on, the system will use two different ways of presenting that information to the
			current action:
				- "allDependingInput": which will have a list of folders to where all the output of
				the parent actions is.
				- "allDependingInput.actionName": To specifically access the folders of inputs
				produced by one specific action, if it is that that action was given a name
				in the configuration file.
				I am going to use JSON configuration for this system.
				Because I love JSON, and it is much simpler than anything else.
		- Remember that this input file with the configuration of the input to a task
		is generated by the system, since the system is the only one who knows where many of
		this paths are at run time.

3.1.2 What does the system do once a workflow has been submitted:
	3.1.2.1 Keeps track of the submission and update statistics on the submissions.
		- Here I need to define the data model that we will use to represent the submissions and
		the statistics on them, in a way that is useful to be queried by others.  Keep in mind
		that one important thing is to be able to access it as time series data too.
		- One approach that I can follow is simply define the most common usage algorithms that
		will be created, and make an API to access the data, and based on the calls to the API
		then define the data model that will make those calls more efficient to handle, by making
		analysis on run time and frequency of those calls by the algorithms.
	3.1.2.2 Keeps track of the provenance of each dataset.
		- This one is a difficult task. First of all, actions needs to be safe
		to work on this environment. An action is safe if given the same input, it always 
		produces the same output.  Then, the provenance of a dataset is sufficiently defined
		by the parent action and its inputs.  In turn, those inputs could be defined in terms
		of their parent actions and their inputs too. If the assumptions are satisfied, we 
		can say that two datasets are equal if their provenances are equal. This assumption
		needs to be tested on real life to see if the usage of our assumptions is acceptable
		in real life.
	3.1.2.3 An algorithm determines which intermediate outputs from the current workflow submission
	will be stored to the cache.  Now, this here becomes interesting, because I want to separate the
	storing to the cache from the deleting from the cache (if possible)
		3.1.2.3.1 Exploring the option of separating deletion and storing.  Storing needs to be
		done at workflow processing time, because that is the time at which I will have access to the
		data that needs to be stored.  The problem
		3.1.2.3.2 Exploring the option of doing deletion and storing both at workflow processing time.
		I can send to store at workflow processing time by adding an FS action to the generated Oozie
		workflow that attempts to store. If there is no space to store, then I can't
		store, but that's it.  If I want to delete at workflow processing time, I have to be sure that
		no workflow currently submitted to Oozie had a dependency on the dataset I want to delete,
		otherwise, I have to wait until that dependency is removed, and then delete.  The way to do this better
		seems to be: first, send the file to delete to a deletion datastructure that as soon as a file 
		lock is released, it gets notified, and then it places a lock on the file and submits
		a delete action as a workflow to Oozie, and then it releases lock and deletes file registry
		as soon as the deletion action is performed.
			- First thoughts: This is the way to go.
			- An interesting problem to consider here is: how do the real available storage space and the reported
			available storage space may differ so that algorithm decisions of storing space might not be carried
			because there is no sufficient storage. I should make it work so that the available storage space
			input to the algorithm is always less than the real available storage space, and in that way, I solve
			the problem of failure of storing data.  In order to do that, the way to do it is the following:
			Keep two variables: the real system storage available space that is updated each time an FS deletion
			action or FS storage action is executed; the potential system storage available space, which will
			be updated into reduction each time a user submits a workflow, and the decision algorithm decides
			to store a dataset.  The problem with this approach is that we have no clue of how big a dataset
			will be until it is actually produced and stored.  Hence, I will have to deal with uncertainty here
			want it or not, but deal with it we will.  Also, maybe, another solution to the dealing with uncertain-
			ty, in order to have the algorithm as efficient as possible, will be to have it consider that
			the available space is only 95% of the actual available space.  The 95% number should be determined
			by what history says is best.
			
	3.1.2.4 An Oozie workflow generator and submitter that executes the workflow submitted by the system.
	It takes as input the workflow submitted by the user, as well as the output produced by the decision algorithm 
	and then it generates a workflow that executes the DAG of the user, as well as has added FS store actions that
	will also be executed which will essentially store outputs to either the cache or the user storage space.
	
	3.1.2.5 A deletion workflow generator that is working all the time, but that is triggered not by the algorithm
	execution, but by the deletion queue, whenever a lock to a dataset to be deleted is processed.
	
	3.1.2.6 Keeping track of all the FS actions submitted to Oozie in order to update locks to files to 
	delete, as well as available storage space variables.
		  
#This points below might change depending on the need to do them.
4. Create a TODO list based on the design of the system above.
6. Create the Accounting Module. Design the data modeling of what is needed
for accounting. Probably use No-sql database.
7. Create the Execution Module that submits the workflows to Oozie.
8. Create the Decision Module that reads from the Accounting Module.
9. Create the Actuator Module (if needed).